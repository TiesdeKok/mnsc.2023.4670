{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'a_6b_parse_filings'   \n",
    "PROJECT = 'covid-empirical'\n",
    "PYTHON_VERSION = '3.9'\n",
    "USER = 'linuxBox' ## Note, this notebook is designed to run on Linux.\n",
    "CONDA_ENVIRONMENT = 'covid-empirical'\n",
    "USE_EXTERNAL_PIPELINE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run preamble script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i preamble.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "import unidecode\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough clean and deal with the style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_style(style_string:str) -> str:\n",
    "    style_list = style_string.split(';')\n",
    "    style_dict = {}\n",
    "    for style_item in style_list:\n",
    "        style_v = style_item.split(':')[-1]\n",
    "        ## Check for font size\n",
    "        if 'font-size' in style_item:\n",
    "            font_size = ''\n",
    "            if style_v == 'large':\n",
    "                font_size = 'large'\n",
    "            else:\n",
    "                try:\n",
    "                    size = int(style_v[:2])\n",
    "                    if size > 12:\n",
    "                        font_size = 'large'\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if font_size:\n",
    "                style_dict['font-size'] = 'large'\n",
    "\n",
    "        ## Check for bold\n",
    "        if 'font-weight' in style_item:\n",
    "            if style_v in ['bold', 'bolder']:\n",
    "                style_dict['font-weight'] = 'bold'\n",
    "            elif style_v[0].isdigit():\n",
    "                style_v = int(style_v)\n",
    "                if style_v > 400:\n",
    "                    style_dict['font-weight'] = 'bold'\n",
    "\n",
    "        ## Check for italic\n",
    "        if 'font-style' in style_item:\n",
    "            if style_v == 'italic':\n",
    "                style_dict['font-style'] = 'italic'\n",
    "                \n",
    "        ## Check for underline (but treat as italic for ease of coding below)\n",
    "        if 'text-decoration' in style_item:\n",
    "            if style_v == 'underline':\n",
    "                style_dict['font-style'] = 'italic'\n",
    "\n",
    "    new_style_string = ''\n",
    "    for k,v in style_dict.items():\n",
    "        new_style_string += f'{k}:{v}; '\n",
    "    new_style_string =  new_style_string.strip()\n",
    "    \n",
    "    return new_style_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_conv_list = [\n",
    "    {\n",
    "        'name' : 'bold',\n",
    "        'names':  ['b', 'B', 'strong'],\n",
    "        'style_to_add' : 'font-weight:bold;'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'italic',\n",
    "        'names':  ['i', 'I', 'em', 'u', 'U'],\n",
    "        'style_to_add' : 'font-style:italic;' \n",
    "    },\n",
    "    {\n",
    "        'name' : 'size',\n",
    "        'names':  ['h1', 'h2', 'h3', 'H1', 'H2', 'H3'],\n",
    "        'style_to_add' : 'font-size:large;' \n",
    "    },\n",
    "]\n",
    "\n",
    "style_chars = []\n",
    "for x in style_conv_list:\n",
    "    style_chars += x['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_html(snippet:str, attrs_to_keep:list = [], verbose:int = 0, remove_tables:bool = True) -> tuple:\n",
    "\n",
    "    ## Clean style\n",
    "    snippet = re.sub('(<style>.*?</style>)', ' ', snippet, flags = re.DOTALL)\n",
    "    snippet = snippet.replace('\\n', ' ')\n",
    "\n",
    "    ## Clean iXBRL at the start\n",
    "    snippet = re.sub('(<ix:header>.*?</ix:header>)', ' ', snippet, flags = re.DOTALL)\n",
    "\n",
    "    ## Clean inline XBRL in notes\n",
    "    for i in range(10):\n",
    "        if 'ix:' in snippet:\n",
    "            snippet = re.sub('(<ix:.*?>)(.*?)(</ix:.*?>)', r'\\2', snippet, flags = re.DOTALL)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    ## Remove HTML comments\n",
    "    snippet = re.sub('(<!--.*?-->)', ' ', snippet, flags = re.DOTALL)\n",
    "\n",
    "    ### Fix whitespace\n",
    "    snippet = snippet.replace('&nbsp;', ' ')\n",
    "    \n",
    "    ## Remove unused HTML elements\n",
    "    for char in style_chars:\n",
    "        snippet = re.sub(f'<{char}></{char}>', ' ', snippet)\n",
    "\n",
    "    ### Process HTML features such as attributes and styles\n",
    "\n",
    "    snippet_tree = BeautifulSoup(snippet, \"html\")\n",
    "\n",
    "    for element in snippet_tree():\n",
    "        ## Move HTML styling through names to the style attribute\n",
    "        for style_conv_item in style_conv_list:\n",
    "            match = False\n",
    "            if element.name in style_conv_item['names']:\n",
    "                match = True\n",
    "            if not match:\n",
    "                for name in style_conv_item['names']:\n",
    "                    if element.find_parents(name=name):\n",
    "                        match = True\n",
    "            \n",
    "            ## Certain elements should never be a style to avoid conflict with the TOC\n",
    "            if element.name in ['a', 'div']:\n",
    "                match = False\n",
    "            \n",
    "            if match:\n",
    "                existing_style = ''\n",
    "                if element.attrs:\n",
    "                    if 'style' in element.attrs:\n",
    "                        existing_style = element.attrs['style']\n",
    "                element.attrs['style'] = existing_style + style_conv_item['style_to_add']\n",
    "        \n",
    "        ## Deal with attributes\n",
    "        if element.attrs:\n",
    "            tmp_dict = copy.deepcopy(element.attrs)\n",
    "            for k,v in tmp_dict.items():\n",
    "                if element.name in ['table', 'TABLE']: ## Never keep styles for tables\n",
    "                    try:\n",
    "                        del element.attrs[k]\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    ## Deal with styles\n",
    "                    if k == 'style' and 'style' in attrs_to_keep:\n",
    "                        new_style =  parse_style(v)\n",
    "                        if new_style:\n",
    "                            element.attrs[k] = new_style\n",
    "                        else:\n",
    "                            try:\n",
    "                                del element.attrs[k]\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                    if element.name in ['a', 'div']:\n",
    "                        if k not in ['href', 'id', 'name']:\n",
    "                            try:\n",
    "                                del element.attrs[k]\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                    ## Remove unwanted styles\n",
    "                    if k not in attrs_to_keep:\n",
    "                        try:\n",
    "                            del element.attrs[k]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "    snippet = str(snippet_tree).strip()\n",
    "    \n",
    "    ## Replace break chars with newline\n",
    "    for break_char in ['<br/>', '<br>', '<BR>', '<BR/>']:\n",
    "        snippet = snippet.replace(break_char, '\\n')\n",
    "    \n",
    "    ## Remove page numbers\n",
    "    snippet = re.sub('''<\\w+?>\\s*?\\d{1,3}\\s*?</\\w+?>''', ' ', snippet)\n",
    "    \n",
    "    ### Get rid of weird HTML characters\n",
    "    snippet = html.unescape(snippet)\n",
    "\n",
    "    ### Get rid of control characters\n",
    "    snippet = unidecode.unidecode(snippet)\n",
    "\n",
    "    ## Deal with tables\n",
    "    tables_of_contents = ''\n",
    "    if remove_tables:\n",
    "        snippet = re.sub('<TABLE>', '<table>', snippet)\n",
    "        snippet = re.sub('</TABLE>', '</table>', snippet)\n",
    "        split_ele = re.split('(?:<table>)', snippet)\n",
    "        new_str, num_tables_removed = '', 0\n",
    "        for i, ele in enumerate(split_ele):\n",
    "            include_table = False\n",
    "            tmp = ele.split('</table>')\n",
    "            if len(tmp) > 1:\n",
    "                table_str_raw, rest_str = tmp[0], tmp[1]\n",
    "                table_str = ' '.join([x for x in re.sub('</*.*?>', ' ', table_str_raw).split(' ') if x])\n",
    "\n",
    "                is_toc = False\n",
    "                if table_str:\n",
    "                    ## Check whether the table is the table of contents\n",
    "                    if table_str_raw.count('href') > 10:\n",
    "                        if 'factors' in table_str.lower() or 'item' in table_str.lower() or 'analysis' in table_str.lower():\n",
    "                            tables_of_contents += table_str_raw\n",
    "                            is_toc = True\n",
    "                            if verbose > 0:\n",
    "                                print('Table of content found')\n",
    "\n",
    "                    ## Check if the table has more than 15% numbers\n",
    "                    if not is_toc:\n",
    "                        num_digit = len(re.findall('\\d', table_str))\n",
    "                        num_alphanum = len(re.findall('\\w', table_str))\n",
    "\n",
    "                        if num_alphanum:\n",
    "                            perc_digit = num_digit / num_alphanum\n",
    "\n",
    "                            if perc_digit <= 0.15:\n",
    "                                include_table = True\n",
    "\n",
    "                if include_table:\n",
    "                    ret_str = table_str_raw + rest_str\n",
    "                else:\n",
    "                    ret_str = rest_str\n",
    "                    num_tables_removed += 1\n",
    "                    if verbose > 1:\n",
    "                        print(f'Table removed with perc: {perc_digit}%')\n",
    "            else:\n",
    "                ret_str = ele\n",
    "\n",
    "            new_str += ret_str\n",
    "        \n",
    "        ## Overwrite string with the string without tables\n",
    "        snippet = new_str\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f'Number of tables removed: {num_tables_removed}')\n",
    "\n",
    "    ## Remove duplicate whitespaces\n",
    "    for ws in [' ', '\\n']:\n",
    "        snippet = ws.join([x for x in snippet.split(ws) if x]).strip() \n",
    "        \n",
    "    ## Return\n",
    "    return snippet, tables_of_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_str(snippet:str, keep_linebreak:bool = False) -> str:  \n",
    "    ## Get rid of HTML\n",
    "    snippet = ' '.join([x for x in re.sub('</*.*?>', ' ', snippet).split(' ') if x])\n",
    "\n",
    "    ### Final remove duplicate whitespace and get rid of newline characters\n",
    "    if keep_linebreak:\n",
    "        for ws in [' ', '\\n']:\n",
    "            snippet = ws.join([x for x in snippet.split(ws) if x]).strip() \n",
    "    else:\n",
    "        snippet = ' '.join([x for x in snippet.split() if x]).strip()\n",
    "    \n",
    "    return snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the sections using the Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the table of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_toc_items(toc:str, cleaner_html:str) -> pd.DataFrame:\n",
    "    if not toc:\n",
    "        raise Exception(\"No table of contents was found...\")\n",
    "        \n",
    "    toc_items = []\n",
    "    for row in re.findall('<tr>(.*?)</tr>', toc, flags=re.DOTALL):\n",
    "        hrefs_found = re.findall('(<a.*?</a>)', row, flags=re.DOTALL)\n",
    "        if hrefs_found:\n",
    "            href_found = hrefs_found[0]\n",
    "            toc_label = ' '.join([x for x in re.findall('>(.*?)<', row, flags=re.DOTALL) if x])\n",
    "            toc_label = toc_label.strip()\n",
    "            toc_label = re.sub('\\s\\d+?$', '', toc_label)\n",
    "            toc_label = ' '.join([x for x in toc_label.split() if x])\n",
    "            if toc_label:\n",
    "                item_number = ''\n",
    "                tmp = re.findall('[iI]tem\\s+(\\w+)\\W*', row)\n",
    "\n",
    "                if tmp:\n",
    "                    item_number = tmp[0]\n",
    "\n",
    "                href = ''\n",
    "                tmp = re.findall('href=[\\'\\\"](.*?)[\\'\\\"]', href_found, flags=re.DOTALL)\n",
    "                if tmp:\n",
    "                    href = tmp[0] \n",
    "\n",
    "                if '#' in href:\n",
    "                    href = '#' + '#'.join(href.split('#')[1:])  \n",
    "                    toc_items.append({\n",
    "                        'label' : toc_label,\n",
    "                        'number' : item_number,\n",
    "                        'href' : href\n",
    "                    })\n",
    "\n",
    "    if not toc_items:\n",
    "        raise Exception(\"No table of contents was found...\")\n",
    "        \n",
    "    toc_df = pd.DataFrame(toc_items)\n",
    "\n",
    "    ## Some companies, especially for 10Qs might split up the A tags for some reason, this joins them\n",
    "    new_df = pd.DataFrame()\n",
    "    for href, df in toc_df.groupby('href'):\n",
    "        df = df.drop_duplicates()\n",
    "        new_label = ' '.join(df.label.values)\n",
    "        df.iloc[0]['label'] = new_label\n",
    "        new_df = new_df.append(df.iloc[0])\n",
    "\n",
    "    new_df = new_df.sort_index() ## This is quite important, as the order is nescessary\n",
    "    toc_df = new_df\n",
    "    \n",
    "    ## This makes sure only valid HREFs are included\n",
    "    valid_hrefs = []\n",
    "    for href in toc_df.href.to_list():\n",
    "        search_id = f'''=\"{href.split('#')[-1]}\"'''\n",
    "\n",
    "        if search_id in cleaner_html:\n",
    "            valid_hrefs.append(href)\n",
    "    toc_df = toc_df[toc_df.href.isin(valid_hrefs)]\n",
    "\n",
    "    ## Identify end\n",
    "    toc_df['end'] = toc_df['href'].shift(-1)\n",
    "    \n",
    "    if len(toc_df) < 3:\n",
    "        raise Exception(\"No table of contents was found...\")\n",
    "    \n",
    "    return toc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify how the table of content is linking to the sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_toc_anchors(toc_df:pd.DataFrame, cleaner_html:str) -> str:\n",
    "    first_id = toc_df.iloc[0]['href'].replace('#', '')\n",
    "    second_id = toc_df.iloc[1]['href'].replace('#', '')\n",
    "\n",
    "    search_regex = '''(<[\\w\\s:'\";=-]+?[a-zA-Z]{1,6}=\"''' + first_id + '''\"[\\w\\s:'\";=-]*?>)'''\n",
    "    search_hits = re.findall(search_regex, cleaner_html)\n",
    "    valid_anchor_found = False\n",
    "    anchor_template =  ''\n",
    "    if search_hits:\n",
    "        search_hit = search_hits[0]\n",
    "        if search_hit:\n",
    "            anchor_name = search_hit.split()[0].replace('<', '')\n",
    "            tmp = re.findall(' (\\w+?)=\"{}\"'.format(first_id), search_hit)\n",
    "            if tmp:\n",
    "                anchor_id_attr = tmp[0]\n",
    "\n",
    "                anchor_template = '<' + anchor_name + '''[\\w\\s:'\";=-]+?''' + anchor_id_attr + '''=\"{}\"[\\w\\s:'\";=-]*?>'''\n",
    "\n",
    "                if re.findall(anchor_template.format(first_id), cleaner_html):\n",
    "                    if re.findall(anchor_template.format(second_id), cleaner_html):\n",
    "                        valid_anchor_found = True\n",
    "    if not valid_anchor_found:\n",
    "        raise Exception('No valid anchors found...')\n",
    "        \n",
    "    return anchor_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(toc_df:pd.DataFrame, anchor_template:str, cleaner_html:str) -> list:\n",
    "    section_list = []\n",
    "\n",
    "    ## Pretext\n",
    "    first_anchor_regex = anchor_template.format(toc_df.iloc[0]['href'].replace('#', ''))\n",
    "    raw_section_text = re.split(first_anchor_regex, cleaner_html)[0]\n",
    "\n",
    "    section_list.append({\n",
    "        'label' : 'pretext',\n",
    "        'number' : '',\n",
    "        'raw_text' : raw_section_text\n",
    "    })\n",
    "\n",
    "\n",
    "    for index, row in toc_df.iterrows():\n",
    "        if index < len(toc_df)-1: \n",
    "            start, end = anchor_template.format(row['href'].replace('#', '')), anchor_template.format(row['end'].replace('#', ''))\n",
    "            tmp_start = re.split(start, cleaner_html)[-1]\n",
    "            raw_section_text = re.split(end, tmp_start)[0]\n",
    "\n",
    "        else:\n",
    "            start = anchor_template.format(row['href'].replace('#', ''))\n",
    "            raw_section_text = re.split(start, cleaner_html)[1]\n",
    "\n",
    "        section_list.append({\n",
    "            'label' : row['label'],\n",
    "            'number' : row['number'],\n",
    "            'raw_text' : raw_section_text\n",
    "        })\n",
    "        \n",
    "    return section_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with the headers and remove HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_out_text_and_header(section_list:list) -> list:\n",
    "    valid_text_endings = ['.', '?', '!', ':', \"'\", '\"']\n",
    "    filing_sub_section_list = []\n",
    "    for section_i, section_item in enumerate(section_list):\n",
    "        raw_text = section_item['raw_text']\n",
    "\n",
    "        ## Fix page breaks splitting up paragraphs\n",
    "        raw_text = re.sub('''([\\w,;])( *<[a-zA-Z\\\\/<>\\s-]+?<hr[\\w\\s:'\";=\\\\/-]*?>[a-zA-Z\\\\/<>\\s]+?> *)([\\w])''', r'\\1 \\3', raw_text)\n",
    "\n",
    "        ## Replace divs, tr with line breaks\n",
    "        for name in ['div', 'tr', 'p', 'html']:\n",
    "            raw_text = raw_text.replace(f'<{name}>', '\\n')\n",
    "            raw_text = raw_text.replace(f'</{name}>', '\\n')\n",
    "\n",
    "        for name in ['div', 'p']:\n",
    "            raw_text = re.sub(f'''<{name}([\\w\\s:'\";=\\\\/-]*?)>''', r'\\n<html_ele\\1>', raw_text)\n",
    "\n",
    "\n",
    "        raw_text = '\\n'.join([x for x in raw_text.split('\\n') if x])\n",
    "\n",
    "        ## Deal with all other HTML relements\n",
    "        unique_html_elements = [x for x in set(re.findall('</*(.*?)>', raw_text)) if '=' not in x]\n",
    "        for name in unique_html_elements:\n",
    "            raw_text = raw_text.replace(f'<{name}', '<html_ele')\n",
    "            raw_text = raw_text.replace(f'</{name}>', '</html_ele>')\n",
    "\n",
    "        ## Force inject newlines for headers that are joined with the text\n",
    "        tmp_list = [x for x in raw_text.split('\\n') if x.strip(' ')]\n",
    "        tmp_list_2 = []\n",
    "        for item in tmp_list:\n",
    "            if 'style=\"' in item:\n",
    "                item_tmp = re.sub('(<html_ele>)(.*?)(</html_ele>)', r' \\2 ', item).strip()\n",
    "                sub_items = item_tmp.split('<html_ele style=\"')\n",
    "\n",
    "                first_item = '<html_ele style=\"'.join(sub_items[:2])\n",
    "                num_char_pre = len(sub_items[0].strip())\n",
    "                if num_char_pre < 4:\n",
    "                    item = re.sub('(<html_ele style=\".*?>)(.*?)(</html_ele>)', r'\\1\\2\\3\\n', first_item)\n",
    "\n",
    "                    ## Sometimes there might be multiple styles later on, which are non-headers by default. But we don't want to lose them. \n",
    "                    if len(sub_items) > 2:\n",
    "                        text_to_add = '<html_ele style=\"' + '<html_ele style=\"'.join(sub_items[2:])\n",
    "                        text_to_add = re.sub(' style=\".*?\"', '', text_to_add) ## Remove style as not a header\n",
    "                        item += text_to_add\n",
    "                else:\n",
    "                    ## Not a header, so remove style\n",
    "                    item = re.sub(' style=\".*?\"', '', item)    \n",
    "\n",
    "            tmp_list_2.append(item)\n",
    "\n",
    "        raw_text = '\\n'.join(tmp_list_2)\n",
    " \n",
    "        raw_text = raw_text.replace('</html_ele>', ' ')\n",
    "        raw_text = raw_text.replace('<html_ele>', ' ')\n",
    "        raw_text = raw_text.replace('<html_ele>', ' ')\n",
    "\n",
    "        raw_text = ' '.join([x for x in raw_text.split(' ') if x])\n",
    "\n",
    "        tmp_list = [x for x in raw_text.split('\\n') if x.strip(' ')]\n",
    "\n",
    "        sub_section_list = []\n",
    "        for _i, item in enumerate(tmp_list):\n",
    "            ss_dict = {\n",
    "                'section_label': section_item['label'],\n",
    "                'section_number' : section_item['number'],\n",
    "                'section_i' : section_i,\n",
    "                'type' : 'text',\n",
    "                'clean_text' : ' '.join([x for x in re.sub('</*.*?>', ' ', item).split() if x])\n",
    "            }\n",
    "\n",
    "            ## Attempt to fix cases where there are special characters at the front\n",
    "            ss_dict['clean_text'] = re.sub('^[\\W\\d]*', '', ss_dict['clean_text']).strip()\n",
    "\n",
    "            if len(ss_dict['clean_text']) > 3:\n",
    "\n",
    "                if 'html_ele style=' in item:\n",
    "                    if 'font-style:italic' in item:\n",
    "                        ## Sometimes italic can occur in the text, which is not a sub-header. So I require a preceding \\n\n",
    "                        if _i > 0:\n",
    "                            ss_dict['type'] = 'sub-header'\n",
    "\n",
    "                    if 'font-weight:bold' in item:\n",
    "                        if 'font-style:italic' in item:\n",
    "                            ss_dict['type'] = 'sub-header' ## If italic and bold --> subheader\n",
    "                        else:\n",
    "                            ss_dict['type'] = 'header'\n",
    "                    if 'font-size:large' in item:\n",
    "                        ss_dict['type'] = 'header'\n",
    "\n",
    "                if len(ss_dict['clean_text']) > 3:\n",
    "\n",
    "                    # But text should be longer than 50 otherwise it should be unknown\n",
    "                    if ss_dict['type'] == 'text' and len(ss_dict['clean_text']) < 50:\n",
    "                        ss_dict['type'] = 'unknown'\n",
    "\n",
    "                    # Valid text should start with a capital letter and end with an ending char\n",
    "                    if ss_dict['type'] == 'text' and not (ss_dict['clean_text'][0].isupper() and ss_dict['clean_text'][-1] in valid_text_endings):\n",
    "                        ss_dict['type'] = 'unknown'\n",
    "\n",
    "                    # If starts with a bracket, consider unknown\n",
    "                    if ss_dict['clean_text'][0] in ['<', '(', '[']:\n",
    "                        ss_dict['type'] = 'unknown'\n",
    "\n",
    "                    sub_section_list.append(ss_dict)\n",
    "\n",
    "        ## This will add an index to enable recreation of sorting\n",
    "\n",
    "        for i, item in enumerate(sub_section_list):\n",
    "            item['i'] = i\n",
    "\n",
    "        filing_sub_section_list += sub_section_list\n",
    "\n",
    "    return filing_sub_section_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_final(filing_sub_section_list:list) -> pd.DataFrame:\n",
    "    \n",
    "    ## ---------------------------------------------------\n",
    "    ### Deal with cases where the TOC contains subsections\n",
    "    ## ---------------------------------------------------\n",
    "    \n",
    "    new_list = []\n",
    "    prev_label = ''\n",
    "    label_change_tracker = ''\n",
    "    for item in filing_sub_section_list:\n",
    "        if 'item' in item['section_label'].lower():\n",
    "            prev_label =  item['section_label']    \n",
    "        elif prev_label:\n",
    "            if label_change_tracker != item['section_label']:\n",
    "                ## Inject the header into the list\n",
    "                new_list.append({\n",
    "                    'section_label': prev_label,\n",
    "                    'section_number' : '',\n",
    "                    'section_i' : 0,\n",
    "                    'type' : 'header',\n",
    "                    'clean_text' : item['section_label']\n",
    "                })\n",
    "                label_change_tracker = item['section_label']\n",
    "\n",
    "            item['section_label'] = prev_label\n",
    "\n",
    "        ## Add item\n",
    "        new_list.append(item)\n",
    "        \n",
    "    filing_sub_section_list = new_list\n",
    "    full_df = pd.DataFrame(filing_sub_section_list)\n",
    "    full_df['i'] = full_df.groupby('section_label').cumcount() ## Reset within section index\n",
    "    \n",
    "    ## ----------------------------------------------------\n",
    "    ## Remove identical items that occur more than 10 times\n",
    "    ## ----------------------------------------------------\n",
    "    \n",
    "    full_df['identical_count'] = full_df.groupby('clean_text')['i'].transform(np.size)\n",
    "    full_df = full_df[full_df.identical_count < 11].reset_index(drop=True)\n",
    "    full_df['i'] = full_df.groupby('section_label').cumcount() ## Reset within section index\n",
    "    full_df = full_df.drop(['identical_count', 'test'], axis=1, errors='ignore')\n",
    "\n",
    "    ## ----------------------------------------------------------------------------------------------------\n",
    "    ## Quite frequently valid pieces of text get split because of page breaks, this attempts to rejoin them\n",
    "    ## ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    filing_sub_section_list = full_df.to_dict('records') \n",
    "\n",
    "    valid_text_endings = ['.', '?', '!', \"'\", '\"']\n",
    "    new_list = []\n",
    "    prev_item, skip_tracker, num_dropped = '', '', 0\n",
    "    for i, item in enumerate(filing_sub_section_list):\n",
    "        if i > 0:\n",
    "            if item['type'] == 'unknown' and prev_item['type'] != 'unknown':\n",
    "                if item['clean_text'][0].isupper():\n",
    "                    for next_i in range(3):\n",
    "                        try:\n",
    "                            next_item = filing_sub_section_list[i + 1 + next_i]\n",
    "                            next_next_item = filing_sub_section_list[i + 2 + next_i]\n",
    "                            if not skip_tracker and next_item['type'] == 'unknown' and next_next_item['type'] != 'unknown' and next_item['clean_text'][-1] in valid_text_endings:\n",
    "                                skip_tracker = next_item['section_number'] + next_item['type'] + next_item['clean_text']\n",
    "                                item['clean_text'] = item['clean_text'].strip() + ' ' + next_item['clean_text'].strip()\n",
    "                                if item['clean_text'][0].isupper() and item['clean_text'][-1] in valid_text_endings and len(item['clean_text']) > 50:\n",
    "                                    item['type'] = 'text' \n",
    "                                break\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "\n",
    "        if item['section_number']+item['type']+item['clean_text'] != skip_tracker:    \n",
    "            new_list.append(item)\n",
    "        else:\n",
    "            num_dropped += 1\n",
    "            skip_tracker = ''\n",
    "\n",
    "        prev_item = item \n",
    "        \n",
    "    filing_sub_section_list = new_list\n",
    "\n",
    "    ## ----------------------\n",
    "    ## Create final dataframe\n",
    "    ## ----------------------\n",
    "    \n",
    "    full_df = pd.DataFrame(filing_sub_section_list)\n",
    "\n",
    "    full_df['i'] = full_df.groupby('section_label').cumcount() ## Reset within section index\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combo function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_for_filing(link:str, raw_html:str, parent_store_loc:str, retrn:bool = True, force_overwrite:bool = False) -> tuple:\n",
    "    ## Setup\n",
    "    parent_store_loc = Path(parent_store_loc)\n",
    "    filing_dict = functions.extract_data_edgar_link(link)\n",
    "    \n",
    "    folder_full = parent_store_loc / 'full' / filing_dict['cik_padded']\n",
    "    folder_split = parent_store_loc / 'split' / filing_dict['cik_padded']\n",
    "    filename = filing_dict['uniqueID'] + '.json.gz'\n",
    "    \n",
    "    ## Check if exists:\n",
    "    if not force_overwrite:\n",
    "        if folder_split.exists():\n",
    "            if (folder_split / filename).exists():\n",
    "                return True, 'Already exists'\n",
    "    \n",
    "    try:\n",
    "        ## Rough clean of HTML and get TOC tables\n",
    "        cleaner_html, toc = sanitize_html(raw_html, attrs_to_keep=['href', 'style', 'id', 'name'], remove_tables=True, verbose = 0)\n",
    "\n",
    "        ## -------------------------------------------------------------\n",
    "        ## Parse full filing without splitting into sections and headers\n",
    "        ## -------------------------------------------------------------\n",
    "\n",
    "        full_filing_clean = html_to_str(cleaner_html)\n",
    "\n",
    "        if not folder_full.exists():\n",
    "            os.mkdir(folder_full)\n",
    "\n",
    "        functions.fast_store_json({'link' : link, 'filing_text' : full_filing_clean}, folder_full / filename)\n",
    "\n",
    "        ## ----------------------------------------\n",
    "        ## Advanced parse with sections and headers\n",
    "        ## ----------------------------------------\n",
    "\n",
    "        ## Set up the TOC for section identification\n",
    "        toc_df = extract_toc_items(toc, cleaner_html)\n",
    "        anchor_template = identify_toc_anchors(toc_df, cleaner_html)\n",
    "\n",
    "        ## Extract sections\n",
    "        section_list =  extract_sections(toc_df, anchor_template, cleaner_html)\n",
    "\n",
    "        ## Parse sections to identify headers, identify valid text, and remove HTML\n",
    "        parsed_sub_section_list = parse_out_text_and_header(section_list)\n",
    "\n",
    "        ## Final clean of the extracted data\n",
    "        final_df = clean_final(parsed_sub_section_list)\n",
    "        final_list = final_df.to_dict('records')\n",
    "\n",
    "        ## Save / Return\n",
    "        ret_dict = {\n",
    "            'link' : link,\n",
    "            'filing_details' : filing_dict,\n",
    "            'filing_list' : final_list\n",
    "        }\n",
    "\n",
    "        if not folder_split.exists():\n",
    "            os.mkdir(folder_split)\n",
    "\n",
    "        functions.fast_store_json(ret_dict, folder_split / filename)\n",
    "\n",
    "        if retrn:\n",
    "            return True, ret_dict\n",
    "        else:\n",
    "            return True, None\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_to_keep=['href', 'style', 'id', 'name']\n",
    "remove_tables=True\n",
    "verbose = 1\n",
    "parent_store_loc = ePipeline / 'out'\n",
    "retrn = False \n",
    "cols_to_keep = ['filingDate', 'reportDate', 'acceptanceDateTime', 'form', 'accessionNumber' , 'primaryDocument', 'size', 'isInlineXBRL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filing meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pipeline = Path.cwd() / '2_pipeline' / 'a_6a_download_raw_filings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_df = pd.read_hdf(download_pipeline / 'out' / 'filing_df.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify done filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_e_pipeline = externalPipelineFolder / 'a_6a_download_raw_filings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_store_loc = download_e_pipeline / 'out' / 'raw'\n",
    "done_list = [x.name.replace('.json.gz', '') for x in raw_store_loc.glob('*.json.gz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_download_df = filing_df[filing_df.uniqueID.isin(done_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81110"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(done_download_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo_func(link:str, raw_store_loc = raw_store_loc, force_overwrite:bool=False):\n",
    "    with warnings.catch_warnings():\n",
    "        try:\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            filing_details = functions.extract_data_edgar_link(link)\n",
    "            file_loc = raw_store_loc / f'''{filing_details['uniqueID']}.json.gz'''\n",
    "            raw_html = functions.fast_load_json(file_loc)['raw']\n",
    "            status, res = run_all_for_filing(link, raw_html, parent_store_loc.as_posix(), retrn = retrn, force_overwrite=force_overwrite)\n",
    "        except Exception as e:\n",
    "            return False, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number done: 81,109, Number todo: 1, which is 0% left to complete.\n",
      "CPU times: user 1.28 s, sys: 524 ms, total: 1.81 s\n",
      "Wall time: 2.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while True:\n",
    "    all_list = done_download_df.uniqueID.to_list()\n",
    "    done_list = [x.name.replace('.json.gz', '') for x in (parent_store_loc / 'full').glob('**/*.json.gz')]\n",
    "    todo_list = list(set(all_list).difference(set(done_list)))\n",
    "    todo_df = done_download_df[done_download_df.uniqueID.isin(todo_list)]\n",
    "    todo_df = todo_df.sample(len(todo_df.index))\n",
    "    todo_link_list = todo_df['link'].to_list()\n",
    "    print(f'Number done: {len(done_list):,}, Number todo: {len(todo_list):,}, which is {len(todo_list) / len(all_list) * 100:.0f}% left to complete.')\n",
    "\n",
    "    res_list = Parallel(n_jobs=20)(delayed(combo_func)(link) for link in todo_link_list)\n",
    "    \n",
    "    if len(todo_link_list) < 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
